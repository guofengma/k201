# Kubernetes 高可用性集群部署

## Kubernetes 版本选择

我们选择当前最新的正式版 1.13.5，此版本继续关注 Kubernetes 的稳定性和可扩展性，其中在群集生命周期管理、存储接口和DNS三个主要功能实现生成环境可用（GA）。具体说明如下：

- 使用kubeadm（GA）简化 Kubernetes 集群管理。它是管理集群生命周期的重要工具，可以实现Kubernetes 集群配置、创建和升级。
- 容器存储接口（CSI）进入GA。通过CSI，Kubernetes 卷变得真正可扩展。第三方存储提供商可以编写与 Kubernetes 互操作而无需触及 Kubernetes 核心代码的插件的机会。
- CoreDNS 现在是 Kubernetes 的默认 DNS 服务器。CoreDNS 是一个通用的、权威的 DNS 服务器，提供与 Kubernetes 向后兼容但可扩展的集成。CoreDNS 相比比以前的 DNS 具有更少的组件，且更强的、更灵活。

并且 Kubernetes 1.13 修复了 Kubernetes API Server 中的一个关键安全问题。

## 准备节点

通过 kubeadm 部署高可用 Kubernetes 集群有两种架构，一种是将数据平面（etcd集群）和控制平面（Kubernetes控制节点）部署在一起，另一种是分开部署，其中部署在一起可以节省服务器，但是数据平面和控制平面耦合在一起，当一台机器故障时，数据平面和控制平面将同时出现问题。

数据平面和控制平面共用节点：

![数据平面和控制平面共用节点](https://github.com/findsec-cn/k201/raw/master/imgs/1/kubeadm-ha-topology-stacked-etcd.png)

数据平面和控制平面不共用节点：

![数据平面和控制平面不共用节点](https://github.com/findsec-cn/k201/raw/master/imgs/1/kubeadm-ha-topology-external-etcd.png)

我们按照数据平面和控制平面共用节点，如下（如果不共用需要多创建三个节点）：

![节点信息](https://github.com/findsec-cn/k201/raw/master/imgs/1/kubeadm-ha-vm.png)

## 升级各节点系统

操作系统我们选择 CentOS 7 最新版（7.6.1810），如果不是最新版，可参考如下升级到最新版。

按如下内容，编辑 /etc/yum.repos.d/CentOS-Base.repo

    # CentOS-Base.repo
    #
    # The mirror system uses the connecting IP address of the client and the
    # update status of each mirror to pick mirrors that are updated to and
    # geographically close to the client.  You should use this for CentOS updates
    # unless you are manually picking other mirrors.
    #
    # If the mirrorlist= does not work for you, as a fall back you can try the
    # remarked out baseurl= line instead.
    #
    #

    [base]
    name=CentOS-$releasever - Base
    #mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=os&infra=$infra
    baseurl=http://mirrors.163.com/centos/7.6.1810/os/$basearch/
    #baseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/
    gpgcheck=1
    gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7

    #released updates
    [updates]
    name=CentOS-$releasever - Updates
    #mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=updates&infra=$infra
    baseurl=http://mirrors.163.com/centos/7.6.1810/updates/$basearch/
    #baseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/
    gpgcheck=1
    gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7

    #additional packages that may be useful
    [extras]
    name=CentOS-$releasever - Extras
    #mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=extras&infra=$infra
    baseurl=http://mirrors.163.com/centos/7.6.1810/extras/$basearch/
    #baseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/
    gpgcheck=1
    gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7

    #additional packages that extend functionality of existing packages
    [centosplus]
    name=CentOS-$releasever - Plus
    #mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=centosplus&infra=$infra
    baseurl=http://mirrors.163.com/centos/7.6.1810/centosplus/$basearch/
    #baseurl=http://mirror.centos.org/centos/$releasever/centosplus/$basearch/
    gpgcheck=1
    enabled=0
    gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7

升级系统并重启

    yum update -y
    reboot

关闭SELinux，编辑 /etc/sysconfig/selinux，设置 SELINUX=disabled

    setenforce 0

配置内核参数

    cat <<EOF >  /etc/sysctl.d/k8s.conf
    net.bridge.bridge-nf-call-ip6tables = 1
    net.bridge.bridge-nf-call-iptables = 1
    net.ipv4.ip_forward = 1
    vm.swappiness = 0
    EOF
    sysctl --system

Kubernetes v1.8+ 要求关闭系统 Swap，请在所有节点利用以下指令关闭：

    swapoff -a && sysctl -w vm.swappiness=0

    vi /etc/fstab
    注释swap相关的行

设置各节点主机名：

    hostnamectl set-hostname k8s-m1
    hostnamectl set-hostname k8s-m2
    hostnamectl set-hostname k8s-m3
    hostnamectl set-hostname k8s-s1
    hostnamectl set-hostname k8s-s2

修改hosts，添加如下行：

    192.168.115.116 k8s-m1
    192.168.115.117 k8s-m2
    192.168.115.118 k8s-m3
    192.168.115.119 k8s-s1
    192.168.115.120 k8s-s2

## 配置所有节点加载 ipvs 相关模块（kube-proxy 使用 ipvs 模式）

加载相关内核模块

    cat > /etc/sysconfig/modules/ipvs.modules << EOF
    #!/bin/bash
    modprobe -- ip_vs
    modprobe -- ip_vs_rr
    modprobe -- ip_vs_wrr
    modprobe -- ip_vs_sh
    modprobe -- nf_conntrack_ipv4
    EOF

    chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4

安装相关软件包

    yum install -y ipset ipvsadm

## 配置ssh key认证

在主节点1上执行生成公钥

    ssh-keygen

拷贝 /root/.ssh/id_rsa.pub 内容到主节点2和主节点3上

    vi /home/centos/.ssh/authorized_keys

## 所有节点安装 Docker

推荐安装 1.11.1, 1.12.1, 1.13.1, 17.03, 17.06, 17.09, 18.06，但是18.06+是未经测试的，不推荐使用。在此选择18.06.3，此版本刚刚修复了RunC容器逃逸漏洞(CVE-2019-5736)。

添加yum仓库配置 /etc/yum.repos.d/docker.repo，内容如下：

    [docker-ce-stable]
    name=Docker CE Stable - x86_64
    baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable
    enabled=1
    gpgcheck=1
    gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

安装并启动 Docker

    yum install -y docker-ce-18.06.3.ce
    systemctl enable docker && systemctl start docker

## 所有节点安装 kubeadm, kubelet and kubectl （需要翻墙）

kubelet版本要与待安装的Kubernetes版本相同，否则可能会出现一些难以预料的问题。

    cat <<EOF > /etc/yum.repos.d/kubernetes.repo
    [kubernetes]
    name=Kubernetes
    baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
    enabled=1
    gpgcheck=1
    repo_gpgcheck=1
    gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
    exclude=kube*
    EOF

通过 yum 安装软件包

    yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

设置开机自动启动kubelet

    systemctl enable kubelet.service

**如果服务器端不能翻墙，可以使用阿里云镜像站点**

    cat <<EOF > /etc/yum.repos.d/kubernetes.repo
    [kubernetes]
    name=Kubernetes
    baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
    enabled=1
    gpgcheck=0
    repo_gpgcheck=0
    gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
           http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
    EOF

## 使用kubeadm创建高可用集群（数据平面和控制平面放置到一起）

### 创建第一个节点控制节点

创建配置文件 kubeadm-config.yaml：

    apiVersion: kubeproxy.config.k8s.io/v1alpha1
    kind: KubeProxyConfiguration
    mode: "ipvs"
    ---
    apiVersion: kubeadm.k8s.io/v1beta1
    kind: ClusterConfiguration
    kubernetesVersion: v1.13.3
    apiServer:
        certSANs:
        - "10.70.93.138"
    controlPlaneEndpoint: "10.70.93.138:6443"
    etcd:
        local:
            dataDir: /data1/etcd
    networking:
        # This CIDR is a Canal default. Substitute or remove for your CNI provider.
        podSubnet: "10.244.0.0/16"
    controllerManager:
        extraArgs:
            address: 0.0.0.0
    scheduler:
        extraArgs:
            address: 0.0.0.0
    imageRepository: gcr.azk8s.cn/google-containers

> 10.70.93.138 为 APIServer 的负载均衡 IP，6443为负载均衡的端口。如果没有负载均可以通过 HaProxy 自行搭建，参见 [HaProxy负载均衡配置](https://github.com/findsec-cn/k200/raw/master/1.高可用性集群部署/kHaProxy负载均衡配置.md)。如果公司有硬件负载均衡如f5、Netscaler等可以直接使用；如果在各云平台可以使用各云平台的负载均衡（如阿里云的ELB）。
> 如果你不能直接访问gcr.io，需要设置 imageRepository: gcr.azk8s.cn。还需要修改每个节点上的kubelet配置（/etc/systemd/system/kubelet.service.d/10-kubeadm.conf），添加 --pod_infra_container_image=gcr.azk8s.cn/pause:3.1 参数。

### 初始化第一个节点

    kubeadm init --config=kubeadm-config.yaml
    ......
    [addons] Applied essential addon: CoreDNS
    [addons] Applied essential addon: kube-proxy

    Your Kubernetes master has initialized successfully!

    To start using your cluster, you need to run the following as a regular user:

    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config

    You should now deploy a pod network to the cluster.
    Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
    https://kubernetes.io/docs/concepts/cluster-administration/addons/

    You can now join any number of machines by running the following on each node
    as root:

    kubeadm join 10.70.93.138:6443 --token skvqhu.b297uimw0omi26w0 --discovery-token-ca-cert-hash sha256:b3b23ae7aea87baa02eda31f7fdbd2604e4cfa20a9f9c278671816d630f30d22

> 注意：以上(kubeadm join)输出在其他节点加入时会使用，需要妥善保管

### 配置网络节点

在此选择 Canal 网络组件，其他网络组建见：https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/

    export KUBECONFIG=/etc/kubernetes/admin.conf

    kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/canal/rbac.yaml
    kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/canal/canal.yaml

等待第一个节点 pod 都变为运行状态

    kubectl get pod -n kube-system
    NAME                                       READY   STATUS    RESTARTS   AGE
    canal-xp9tm                                3/3     Running   0          79s
    coredns-86c58d9df4-d62mw                   1/1     Running   0          2m37s
    coredns-86c58d9df4-wq26g                   1/1     Running   0          2m37s
    etcd-k8s-m1.novalocal                      1/1     Running   0          2m3s
    kube-apiserver-k8s-m1.novalocal            1/1     Running   0          115s
    kube-controller-manager-k8s-m1.novalocal   1/1     Running   0          2m21s
    kube-proxy-t8x22                           1/1     Running   0          2m37s
    kube-scheduler-k8s-m1.novalocal            1/1     Running   0          2m12s

### 复制证书到其他控制节点

    USER=centos
    CONTROL_PLANE_IPS="192.168.115.117 192.168.115.118"
    for host in ${CONTROL_PLANE_IPS}; do
        scp /etc/kubernetes/pki/ca.crt "${USER}"@$host:
        scp /etc/kubernetes/pki/ca.key "${USER}"@$host:
        scp /etc/kubernetes/pki/sa.key "${USER}"@$host:
        scp /etc/kubernetes/pki/sa.pub "${USER}"@$host:
        scp /etc/kubernetes/pki/front-proxy-ca.crt "${USER}"@$host:
        scp /etc/kubernetes/pki/front-proxy-ca.key "${USER}"@$host:
        scp /etc/kubernetes/pki/etcd/ca.crt "${USER}"@$host:etcd-ca.crt
        scp /etc/kubernetes/pki/etcd/ca.key "${USER}"@$host:etcd-ca.key
        scp /etc/kubernetes/admin.conf "${USER}"@$host:
    done

### 登录第二、三个控制节点移动证书到正确位置

    USER=centos
    mkdir -p /etc/kubernetes/pki/etcd
    mv /home/${USER}/ca.crt /etc/kubernetes/pki/
    mv /home/${USER}/ca.key /etc/kubernetes/pki/
    mv /home/${USER}/sa.pub /etc/kubernetes/pki/
    mv /home/${USER}/sa.key /etc/kubernetes/pki/
    mv /home/${USER}/front-proxy-ca.crt /etc/kubernetes/pki/
    mv /home/${USER}/front-proxy-ca.key /etc/kubernetes/pki/
    mv /home/${USER}/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
    mv /home/${USER}/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
    mv /home/${USER}/admin.conf /etc/kubernetes/admin.conf

### 将第二、三个控制节点加入集群

    # 在第二个控制节点执行
    kubeadm join 10.70.93.138:6443 --token skvqhu.b297uimw0omi26w0 --discovery-token-ca-cert-hash sha256:b3b23ae7aea87baa02eda31f7fdbd2604e4cfa20a9f9c278671816d630f30d22 --experimental-control-plane
    # 在第三个控制节点执行
    kubeadm join 10.70.93.138:6443 --token skvqhu.b297uimw0omi26w0 --discovery-token-ca-cert-hash sha256:b3b23ae7aea87baa02eda31f7fdbd2604e4cfa20a9f9c278671816d630f30d22 --experimental-control-plane

### 将从节点加入集群

    # 在S1节点执行
    kubeadm join 10.70.93.138:6443 --token skvqhu.b297uimw0omi26w0 --discovery-token-ca-cert-hash sha256:b3b23ae7aea87baa02eda31f7fdbd2604e4cfa20a9f9c278671816d630f30d22
    # 在S2节点执行
    kubeadm join 10.70.93.138:6443 --token skvqhu.b297uimw0omi26w0 --discovery-token-ca-cert-hash sha256:b3b23ae7aea87baa02eda31f7fdbd2604e4cfa20a9f9c278671816d630f30d22

## 部署高可用 Kubernetes（使用外部etcd集群）

### 在三个 etcd 节点上配置使用 kubelet 启动 etcd

配置并启动 kubelet

    cat << EOF > /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
    [Service]
    ExecStart=
    ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true
    Restart=always
    EOF

    systemctl daemon-reload
    systemctl restart kubelet

### 为 kubeadm 创建配置文件

使用如下脚本在第一个 etcd 服务器上创建 kubeadm 创建配置文件

    # Update HOST0, HOST1, and HOST2 with the IPs or resolvable names of your hosts
    export HOST0=192.168.115.100
    export HOST1=192.168.115.101
    export HOST2=192.168.115.102

    # Create temp directories to store files that will end up on other hosts.
    mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/

    ETCDHOSTS=(${HOST0} ${HOST1} ${HOST2})
    NAMES=("k8s-m1" "k8s-m2" "k8s-m3")

    for i in "${!ETCDHOSTS[@]}"; do
    HOST=${ETCDHOSTS[$i]}
    NAME=${NAMES[$i]}
    cat << EOF > /tmp/${HOST}/kubeadmcfg.yaml
    apiVersion: "kubeadm.k8s.io/v1beta1"
    kind: ClusterConfiguration
    etcd:
        local:
            serverCertSANs:
            - "${HOST}"
            peerCertSANs:
            - "${HOST}"
            extraArgs:
                initial-cluster: ${NAMES[0]}=https://${ETCDHOSTS[0]}:2380,${NAMES[1]}=https://${ETCDHOSTS[1]}:2380,${NAMES[2]}=https://${ETCDHOSTS[2]}:2380
                initial-cluster-state: new
                name: ${NAME}
                listen-peer-urls: https://${HOST}:2380
                listen-client-urls: https://${HOST}:2379
                advertise-client-urls: https://${HOST}:2379
                initial-advertise-peer-urls: https://${HOST}:2380
    EOF
    done

### 为 etcd 生成 ca

如果你已有CA，那么唯一的操作是将CA crt和 key文件复制到 /etc/kubernetes/pki/etcd/ca.crt 和 /etc/kubernetes/pki/etcd/ca.key。复制这些文件后，继续执行下一步“为每个成员创建证书”。

如果您还没有CA，则在$HOST0（生成kubeadm的配置文件的位置）上运行此命令。

    kubeadm init phase certs etcd-ca

这会创建两个文件：

    /etc/kubernetes/pki/etcd/ca.crt
    /etc/kubernetes/pki/etcd/ca.key

### 为每个成员创建证书

    kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml
    kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml
    kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
    kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
    cp -R /etc/kubernetes/pki /tmp/${HOST2}/
    # 清理不在使用的证书
    find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

    kubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml
    kubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml
    kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
    kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
    cp -R /etc/kubernetes/pki /tmp/${HOST1}/
    find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

    kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml
    kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml
    kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
    kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml

    # 清理无需拷贝的文件ca.key
    find /tmp/${HOST2} -name ca.key -type f -delete
    find /tmp/${HOST1} -name ca.key -type f -delete

### 复制证书和 kubeadm 配置到其他两个节点

    USER=centos
    HOST=${HOST1}
    scp -r /tmp/${HOST}/* ${USER}@${HOST}:
    ssh ${USER}@${HOST}
    USER@HOST $ sudo -Es
    root@HOST $ chown -R root:root pki
    root@HOST $ mv pki /etc/kubernetes/

    USER=centos
    HOST=${HOST2}
    scp -r /tmp/${HOST}/* ${USER}@${HOST}:
    ssh ${USER}@${HOST}
    USER@HOST $ sudo -Es
    root@HOST $ chown -R root:root pki
    root@HOST $ mv pki /etc/kubernetes/

### 确保三个节点存在所有预期文件

所需文件的完整列表$HOST0是：

    /tmp/${HOST0}
    └── kubeadmcfg.yaml
    ---
    /etc/kubernetes/pki
    ├── apiserver-etcd-client.crt
    ├── apiserver-etcd-client.key
    └── etcd
        ├── ca.crt
        ├── ca.key
        ├── healthcheck-client.crt
        ├── healthcheck-client.key
        ├── peer.crt
        ├── peer.key
        ├── server.crt
        └── server.key

$HOST1上：

    $HOME
    └── kubeadmcfg.yaml
    ---
    /etc/kubernetes/pki
    ├── apiserver-etcd-client.crt
    ├── apiserver-etcd-client.key
    └── etcd
        ├── ca.crt
        ├── healthcheck-client.crt
        ├── healthcheck-client.key
        ├── peer.crt
        ├── peer.key
        ├── server.crt
        └── server.key

$HOST2上：

    $HOME
    └── kubeadmcfg.yaml
    ---
    /etc/kubernetes/pki
    ├── apiserver-etcd-client.crt
    ├── apiserver-etcd-client.key
    └── etcd
        ├── ca.crt
        ├── healthcheck-client.crt
        ├── healthcheck-client.key
        ├── peer.crt
        ├── peer.key
        ├── server.crt
        └── server.key

### 在各节点上创建静态 pod 配置

    root@HOST0 $ kubeadm init phase etcd local --config=/tmp/${HOST0}/kubeadmcfg.yaml
    root@HOST1 $ kubeadm init phase etcd local --config=/home/centos/kubeadmcfg.yaml
    root@HOST2 $ kubeadm init phase etcd local --config=/home/centos/kubeadmcfg.yaml

### 在第一个节点上执行，检查群集运行状况

    docker run --rm -it \
    --net host \
    -v /etc/kubernetes:/etc/kubernetes k8s.gcr.io/etcd:3.2.24 etcdctl \
    --cert-file /etc/kubernetes/pki/etcd/peer.crt \
    --key-file /etc/kubernetes/pki/etcd/peer.key \
    --ca-file /etc/kubernetes/pki/etcd/ca.crt \
    --endpoints https://${HOST0}:2379 cluster-health
    ...
    cluster is healthy

### 拷贝 etcd 证书文件到 Kubernetes 控制节点

将以下文件从 etcd 集群中的任何节点复制到 Kubernetes master 节点（如果 etcd 节点即是 Kubernetes master 节点则此不可省略）：

    export CONTROL_PLANE="centos@192.168.115.116"
    +scp /etc/kubernetes/pki/etcd/ca.crt "${CONTROL_PLANE}":
    +scp /etc/kubernetes/pki/apiserver-etcd-client.crt "${CONTROL_PLANE}":
    +scp /etc/kubernetes/pki/apiserver-etcd-client.key "${CONTROL_PLANE}":

替换的值 CONTROL_PLANE 与 user@host 个 Kubernetes master 节点。

### 创建 kubeadm 配置文件

创建kubeadmin配置文件 kubeadm-config.yaml

    apiVersion: kubeadm.k8s.io/v1beta1
    kind: ClusterConfiguration
    kubernetesVersion: v1.13.3
    apiServer:
    certSANs:
    - "10.70.93.138"
    controlPlaneEndpoint: "10.70.93.138:6443"
    etcd:
        external:
            endpoints:
            - https://192.168.115.100:2379
            - https://192.168.115.101:2379
            - https://192.168.115.102:2379
            caFile: /etc/kubernetes/pki/etcd/ca.crt
            certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
            keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
    networking:
        # This CIDR is a Canal default. Substitute or remove for your CNI provider.
        podSubnet: "10.244.0.0/16"
    controllerManager:
    extraArgs:
        address: 0.0.0.0
    scheduler:
    extraArgs:
        address: 0.0.0.0

### 在第一个控制节点上执行

    kubeadm init --config kubeadm-config.yaml

> 注意：以上(kubeadm join)输出在其他节点加入时会使用，需要妥善保管

### 配置网络节点

    export KUBECONFIG=/etc/kubernetes/admin.conf

    kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/canal/rbac.yaml

    kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/canal/canal.yaml

### 在其他两个控制节点上执行 join 命令加入集群

    kubeadm join 10.70.93.138:6443 --token skvqhu.b297uimw0omi26w0 --discovery-token-ca-cert-hash sha256:b3b23ae7aea87baa02eda31f7fdbd2604e4cfa20a9f9c278671816d630f30d22 --experimental-control-plane
    kubeadm join 10.70.93.138:6443 --token skvqhu.b297uimw0omi26w0 --discovery-token-ca-cert-hash sha256:b3b23ae7aea87baa02eda31f7fdbd2604e4cfa20a9f9c278671816d630f30d22 --experimental-control-plane

### 配置从节点加载ipvs相关模块

加载相关内核模块

    cat > /etc/sysconfig/modules/ipvs.modules << EOF
    #!/bin/bash
    modprobe ip_vs
    modprobe ip_vs_rr
    modprobe ip_vs_wrr
    modprobe ip_vs_sh
    modprobe nf_conntrack_ipv4
    EOF

    chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4

上面脚本创建了的/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。

### 将从节点加入集群

    # 在S1节点执行
    kubeadm join 10.70.93.138:6443 --token skvqhu.b297uimw0omi26w0 --discovery-token-ca-cert-hash sha256:b3b23ae7aea87baa02eda31f7fdbd2604e4cfa20a9f9c278671816d630f30d22

    # 在S2节点执行
    kubeadm join 10.70.93.138:6443 --token skvqhu.b297uimw0omi26w0 --discovery-token-ca-cert-hash sha256:b3b23ae7aea87baa02eda31f7fdbd2604e4cfa20a9f9c278671816d630f30d22

## 部署测试应用

### 安装、配置 helm

下载并安装helm

    wget https://storage.googleapis.com/kubernetes-helm/helm-v2.13.0-linux-amd64.tar.gz
    tar xzvf helm-v2.13.0-linux-amd64.tar.gz
    cp linux-amd64/helm /usr/local/bin/
    chmod +x /usr/local/bin/helm

创建 helm 服务账号

    vi helm-service-account.yaml
    # Create a service account for Helm and grant the cluster admin role.
    # It is assumed that helm should be installed with this service account
    # (tiller).
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: tiller
      namespace: kube-system
    ---
    apiVersion: rbac.authorization.k8s.io/v1beta1
    kind: ClusterRoleBinding
    metadata:
      name: tiller
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: cluster-admin
    subjects:
    - kind: ServiceAccount
      name: tiller
      namespace: kube-system

创建账号

    kubectl create -f helm-service-account.yaml

初始化服务器端部署工具tiller

    helm init --service-account tiller

如果不能翻墙可以如下部署tiller

    helm init --skip-refresh --service-account tiller --tiller-image gcr.azk8s.cn/kubernetes-helm/tiller:v2.13.0

等待部署tiller完成

    kubectl -n kube-system get pods
    NAME                               READY     STATUS    RESTARTS   AGE
    ......
    tiller-deploy-56c4cf647b-plfnf     1/1       Running   0          7m

### 使用 helm 部署 wordpress 应用

通过helm部署wordpress应用

    helm install --namespace wordpress --name wordpress --set persistence.enabled=false --set mariadb.master.persistence.enabled=false stable/wordpress

更新wordpress应用通过NodePort访问

    helm upgrade --namespace wordpress --set persistence.enabled=false --set mariadb.master.persistence.enabled=false --set serviceType=NodePort wordpress stable/wordpress

查看部署情况

    kubectl -n wordpress -o wide get pods
    NAME                                   READY     STATUS    RESTARTS   AGE       IP           NODE       NOMINATED NODE
    wordpress-mariadb-0                    1/1       Running   0          20m       10.244.2.7   k8s-m3   <none>
    wordpress-wordpress-75c5698f99-z5zql   1/1       Running   0          7m        10.244.1.2   k8s-m2   <none>

获取服务访问信息

    kubectl get svc --namespace wordpress -w wordpress-wordpress
    NAME                  TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
    wordpress-wordpress   NodePort   10.111.15.23   <none>        80:32554/TCP,443:31949/TCP   13h

可以通过IP+NodePort访问应用

    export NODE_PORT=$(kubectl get --namespace wordpress -o jsonpath="{.spec.ports[0].nodePort}" services wordpress-wordpress)
    export NODE_IP=$(kubectl get nodes --namespace wordpress -o jsonpath="{.items[0].status.addresses[0].address}")

    echo "WordPress URL: http://$NODE_IP:$NODE_PORT/"
    WordPress URL: http://192.168.115.83:32554/

可以通过浏览器访问IP+NodePort验证WordPress是否可以访问。

列出 helm 部署

    helm list
    NAME     	REVISION	UPDATED                 	STATUS  	CHART          	APP VERSION	NAMESPACE
    wordpress	2       	Thu Feb 28 07:09:15 2019	DEPLOYED	wordpress-5.2.6	5.1.0      	wordpress

删除部署

    helm delete wordpress --purge
    release "wordpress" deleted
